{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009041,"end_time":"2023-05-18T10:30:46.149968","exception":false,"start_time":"2023-05-18T10:30:46.140927","status":"completed"},"tags":[]},"source":["# Titanic competition with TensorFlow Decision Forests\n","\n","This notebook will take you through the steps needed to train a baseline Gradient Boosted Trees Model using TensorFlow Decision Forests and creating a submission on the Titanic competition. \n","\n","This notebook shows:\n","\n","1. How to do some basic pre-processing. For example, the passenger names will be tokenized, and ticket names will be splitted in parts.\n","1. How to train a Gradient Boosted Trees (GBT) with default parameters\n","1. How to train a GBT with improved default parameters\n","1. How to tune the parameters of a GBTs\n","1. How to train and ensemble many GBTs"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007644,"end_time":"2023-05-18T10:30:46.165684","exception":false,"start_time":"2023-05-18T10:30:46.158040","status":"completed"},"tags":[]},"source":["# Imports dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T02:23:14.026689Z","iopub.status.busy":"2023-11-08T02:23:14.026205Z","iopub.status.idle":"2023-11-08T02:23:24.819614Z","shell.execute_reply":"2023-11-08T02:23:24.818331Z","shell.execute_reply.started":"2023-11-08T02:23:14.026655Z"},"papermill":{"duration":9.877832,"end_time":"2023-05-18T10:30:56.051537","exception":false,"start_time":"2023-05-18T10:30:46.173705","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-11-08 15:36:54.813148: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-11-08 15:36:55.978340: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-11-08 15:36:55.986570: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-11-08 15:36:58.541107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","WARNING:root:TensorFlow Decision Forests 1.5.0 is compatible with the following TensorFlow Versions: ['2.13.0']. However, TensorFlow 2.13.1 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n"]},{"name":"stdout","output_type":"stream","text":["Found TF-DF 1.5.0\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","\n","import tensorflow as tf\n","import tensorflow_decision_forests as tfdf\n","#Disable warning\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","print(f\"Found TF-DF {tfdf.__version__}\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008393,"end_time":"2023-05-18T10:30:56.068502","exception":false,"start_time":"2023-05-18T10:30:56.060109","status":"completed"},"tags":[]},"source":["# Load dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T02:23:24.822943Z","iopub.status.busy":"2023-11-08T02:23:24.821697Z","iopub.status.idle":"2023-11-08T02:23:24.894210Z","shell.execute_reply":"2023-11-08T02:23:24.893017Z","shell.execute_reply.started":"2023-11-08T02:23:24.822901Z"},"papermill":{"duration":0.080398,"end_time":"2023-05-18T10:30:56.157107","exception":false,"start_time":"2023-05-18T10:30:56.076709","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Moran, Mr. James</td>\n","      <td>male</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>330877</td>\n","      <td>8.4583</td>\n","      <td>NaN</td>\n","      <td>Q</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>McCarthy, Mr. Timothy J</td>\n","      <td>male</td>\n","      <td>54.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>17463</td>\n","      <td>51.8625</td>\n","      <td>E46</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Palsson, Master. Gosta Leonard</td>\n","      <td>male</td>\n","      <td>2.0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>349909</td>\n","      <td>21.0750</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n","      <td>female</td>\n","      <td>27.0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>347742</td>\n","      <td>11.1333</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n","      <td>female</td>\n","      <td>14.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>237736</td>\n","      <td>30.0708</td>\n","      <td>NaN</td>\n","      <td>C</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","5            6         0       3   \n","6            7         0       1   \n","7            8         0       3   \n","8            9         1       3   \n","9           10         1       2   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","5                                   Moran, Mr. James    male   NaN      0   \n","6                            McCarthy, Mr. Timothy J    male  54.0      0   \n","7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n","8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n","9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n","\n","   Parch            Ticket     Fare Cabin Embarked  \n","0      0         A/5 21171   7.2500   NaN        S  \n","1      0          PC 17599  71.2833   C85        C  \n","2      0  STON/O2. 3101282   7.9250   NaN        S  \n","3      0            113803  53.1000  C123        S  \n","4      0            373450   8.0500   NaN        S  \n","5      0            330877   8.4583   NaN        Q  \n","6      0             17463  51.8625   E46        S  \n","7      1            349909  21.0750   NaN        S  \n","8      2            347742  11.1333   NaN        S  \n","9      0            237736  30.0708   NaN        C  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["try:\n","    train_df = pd.read_csv(\"Dataset/train.csv\")\n","    serving_df = pd.read_csv(\"Dataset/test.csv\")\n","except:\n","    train_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n","    serving_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n","\n","train_df.head(10)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008374,"end_time":"2023-05-18T10:30:56.174348","exception":false,"start_time":"2023-05-18T10:30:56.165974","status":"completed"},"tags":[]},"source":["# Prepare dataset\n","\n","We will apply the following transformations on the dataset.\n","\n","1. Tokenize the names. For example, \"Braund, Mr. Owen Harris\" will become [\"Braund\", \"Mr.\", \"Owen\", \"Harris\"].\n","2. Extract any prefix in the ticket. For example ticket \"STON/O2. 3101282\" will become \"STON/O2.\" and 3101282."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T02:23:24.896951Z","iopub.status.busy":"2023-11-08T02:23:24.896176Z","iopub.status.idle":"2023-11-08T02:23:24.936857Z","shell.execute_reply":"2023-11-08T02:23:24.935641Z","shell.execute_reply.started":"2023-11-08T02:23:24.896910Z"},"papermill":{"duration":0.055455,"end_time":"2023-05-18T10:30:56.238539","exception":false,"start_time":"2023-05-18T10:30:56.183084","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","      <th>Ticket_number</th>\n","      <th>Ticket_item</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund Mr Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>21171</td>\n","      <td>A/5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings Mrs John Bradley Florence Briggs Thayer</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","      <td>17599</td>\n","      <td>PC</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen Miss Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>3101282</td>\n","      <td>STON/O2.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle Mrs Jacques Heath Lily May Peel</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","      <td>113803</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen Mr William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>373450</td>\n","      <td>NONE</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                              Name     Sex   Age  SibSp  \\\n","0                            Braund Mr Owen Harris    male  22.0      1   \n","1  Cumings Mrs John Bradley Florence Briggs Thayer  female  38.0      1   \n","2                             Heikkinen Miss Laina  female  26.0      0   \n","3         Futrelle Mrs Jacques Heath Lily May Peel  female  35.0      1   \n","4                           Allen Mr William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked Ticket_number Ticket_item  \n","0      0         A/5 21171   7.2500   NaN        S         21171         A/5  \n","1      0          PC 17599  71.2833   C85        C         17599          PC  \n","2      0  STON/O2. 3101282   7.9250   NaN        S       3101282    STON/O2.  \n","3      0            113803  53.1000  C123        S        113803        NONE  \n","4      0            373450   8.0500   NaN        S        373450        NONE  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["def preprocess(df):\n","    df = df.copy()\n","    \n","    def normalize_name(x):\n","        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n","    \n","    def ticket_number(x):\n","        return x.split(\" \")[-1]\n","        \n","    def ticket_item(x):\n","        items = x.split(\" \")\n","        if len(items) == 1:\n","            return \"NONE\"\n","        return \"_\".join(items[0:-1])\n","    \n","    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n","    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n","    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)                     \n","    return df\n","    \n","preprocessed_train_df = preprocess(train_df)\n","preprocessed_serving_df = preprocess(serving_df)\n","\n","preprocessed_train_df.head(5)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008741,"end_time":"2023-05-18T10:30:56.256397","exception":false,"start_time":"2023-05-18T10:30:56.247656","status":"completed"},"tags":[]},"source":["Let's keep the list of the input features of the model. Notably, we don't want to train our model on the \"PassengerId\" and \"Ticket\" features."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T02:23:24.941170Z","iopub.status.busy":"2023-11-08T02:23:24.940405Z","iopub.status.idle":"2023-11-08T02:23:24.949428Z","shell.execute_reply":"2023-11-08T02:23:24.948285Z","shell.execute_reply.started":"2023-11-08T02:23:24.941123Z"},"papermill":{"duration":0.0219,"end_time":"2023-05-18T10:30:56.287214","exception":false,"start_time":"2023-05-18T10:30:56.265314","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input features: ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked', 'Ticket_number', 'Ticket_item']\n"]}],"source":["input_features = list(preprocessed_train_df.columns)\n","input_features.remove(\"Ticket\")\n","input_features.remove(\"PassengerId\")\n","input_features.remove(\"Survived\")\n","#input_features.remove(\"Ticket_number\")\n","\n","print(f\"Input features: {input_features}\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00868,"end_time":"2023-05-18T10:30:56.305032","exception":false,"start_time":"2023-05-18T10:30:56.296352","status":"completed"},"tags":[]},"source":["# Convert Pandas dataset to TensorFlow Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T02:23:24.951901Z","iopub.status.busy":"2023-11-08T02:23:24.951145Z","iopub.status.idle":"2023-11-08T02:23:25.320966Z","shell.execute_reply":"2023-11-08T02:23:25.319612Z","shell.execute_reply.started":"2023-11-08T02:23:24.951857Z"},"papermill":{"duration":0.390876,"end_time":"2023-05-18T10:30:56.705086","exception":false,"start_time":"2023-05-18T10:30:56.314210","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-11-08 15:37:30.895810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-11-08 15:37:30.896771: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n","Skipping registering GPU devices...\n"]}],"source":["def tokenize_names(features, labels=None):\n","    \"\"\"Divite the names into tokens. TF-DF can consume text tokens natively.\"\"\"\n","    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\n","    return features, labels\n","\n","train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df,label=\"Survived\").map(tokenize_names)\n","serving_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008633,"end_time":"2023-05-18T10:30:56.722976","exception":false,"start_time":"2023-05-18T10:30:56.714343","status":"completed"},"tags":[]},"source":["# Train model with default parameters\n","\n","### Train model\n","\n","First, we are training a GradientBoostedTreesModel model with the default parameters."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T02:23:25.323526Z","iopub.status.busy":"2023-11-08T02:23:25.323043Z","iopub.status.idle":"2023-11-08T02:23:37.976799Z","shell.execute_reply":"2023-11-08T02:23:37.975631Z","shell.execute_reply.started":"2023-11-08T02:23:25.323482Z"},"papermill":{"duration":12.446474,"end_time":"2023-05-18T10:31:09.178415","exception":false,"start_time":"2023-05-18T10:30:56.731941","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[WARNING 23-11-08 15:37:52.7656 NZDT gradient_boosted_trees.cc:1818] \"goss_alpha\" set but \"sampling_method\" not equal to \"GOSS\".\n","[WARNING 23-11-08 15:37:52.7656 NZDT gradient_boosted_trees.cc:1829] \"goss_beta\" set but \"sampling_method\" not equal to \"GOSS\".\n","[WARNING 23-11-08 15:37:52.7656 NZDT gradient_boosted_trees.cc:1843] \"selective_gradient_boosting_ratio\" set but \"sampling_method\" not equal to \"SELGB\".\n","[INFO 23-11-08 15:37:53.4257 NZDT kernel.cc:1243] Loading model from path /tmp/tmphey7svhy/model/ with prefix fd1e14704ed342f2\n","[INFO 23-11-08 15:37:53.4286 NZDT abstract_model.cc:1311] Engine \"GradientBoostedTreesQuickScorerExtended\" built\n","[INFO 23-11-08 15:37:53.4286 NZDT kernel.cc:1075] Use fast generic engine\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fa0419551f0> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: could not get source code\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fa0419551f0> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: could not get source code\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"name":"stdout","output_type":"stream","text":["WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fa0419551f0> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: could not get source code\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","Accuracy: 0.8260869383811951 Loss:0.8608942627906799\n"]}],"source":["model = tfdf.keras.GradientBoostedTreesModel(\n","    verbose=0, # Very few logs\n","    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n","    exclude_non_specified_features=True, # Only use the features in \"features\"\n","    random_seed=1234,\n",")\n","model.fit(train_ds)\n","\n","self_evaluation = model.make_inspector().evaluation()\n","print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009801,"end_time":"2023-05-18T10:31:09.197829","exception":false,"start_time":"2023-05-18T10:31:09.188028","status":"completed"},"tags":[]},"source":["# Train model with improved default parameters\n","\n","Now you'll use some specific parameters when creating the GBT model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T02:23:37.979528Z","iopub.status.busy":"2023-11-08T02:23:37.979051Z","iopub.status.idle":"2023-11-08T02:23:39.600162Z","shell.execute_reply":"2023-11-08T02:23:39.599039Z","shell.execute_reply.started":"2023-11-08T02:23:37.979486Z"},"papermill":{"duration":1.383354,"end_time":"2023-05-18T10:31:10.591287","exception":false,"start_time":"2023-05-18T10:31:09.207933","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[WARNING 23-11-08 15:37:59.4463 NZDT gradient_boosted_trees.cc:1818] \"goss_alpha\" set but \"sampling_method\" not equal to \"GOSS\".\n","[WARNING 23-11-08 15:37:59.4463 NZDT gradient_boosted_trees.cc:1829] \"goss_beta\" set but \"sampling_method\" not equal to \"GOSS\".\n","[WARNING 23-11-08 15:37:59.4464 NZDT gradient_boosted_trees.cc:1843] \"selective_gradient_boosting_ratio\" set but \"sampling_method\" not equal to \"SELGB\".\n","[INFO 23-11-08 15:38:00.5774 NZDT kernel.cc:1243] Loading model from path /tmp/tmpj1v9vz0m/model/ with prefix da67620fb79147b1\n","[INFO 23-11-08 15:38:00.5928 NZDT decision_forest.cc:660] Model loaded with 42 root(s), 2212 node(s), and 10 input feature(s).\n","[INFO 23-11-08 15:38:00.5929 NZDT kernel.cc:1075] Use fast generic engine\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.782608687877655 Loss:1.060815453529358\n"]}],"source":["model = tfdf.keras.GradientBoostedTreesModel(\n","    verbose=0, # Very few logs\n","    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n","    exclude_non_specified_features=True, # Only use the features in \"features\"\n","    \n","    #num_trees=2000,\n","    \n","    # Only for GBT.\n","    # A bit slower, but great to understand the model.\n","    # compute_permutation_variable_importance=True,\n","    \n","    # Change the default hyper-parameters\n","    # hyperparameter_template=\"benchmark_rank1@v1\",\n","    \n","    #num_trees=1000,\n","    #tuner=tuner\n","    \n","    min_examples=1,\n","    categorical_algorithm=\"RANDOM\",\n","    #max_depth=4,\n","    shrinkage=0.05,\n","    #num_candidate_attributes_ratio=0.2,\n","    split_axis=\"SPARSE_OBLIQUE\",\n","    sparse_oblique_normalization=\"MIN_MAX\",\n","    sparse_oblique_num_projections_exponent=2.0,\n","    num_trees=2000,\n","    #validation_ratio=0.0,\n","    random_seed=1234,\n","    \n",")\n","model.fit(train_ds)\n","\n","self_evaluation = model.make_inspector().evaluation()\n","print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009209,"end_time":"2023-05-18T10:31:10.610105","exception":false,"start_time":"2023-05-18T10:31:10.600896","status":"completed"},"tags":[]},"source":["Let's look at the model and you can also notice the information about variable importance that the model figured out"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T02:23:39.601772Z","iopub.status.busy":"2023-11-08T02:23:39.601432Z","iopub.status.idle":"2023-11-08T02:23:39.617896Z","shell.execute_reply":"2023-11-08T02:23:39.616706Z","shell.execute_reply.started":"2023-11-08T02:23:39.601745Z"},"papermill":{"duration":0.038389,"end_time":"2023-05-18T10:31:10.658474","exception":false,"start_time":"2023-05-18T10:31:10.620085","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"gradient_boosted_trees_model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n","=================================================================\n","Total params: 1 (1.00 Byte)\n","Trainable params: 0 (0.00 Byte)\n","Non-trainable params: 1 (1.00 Byte)\n","_________________________________________________________________\n","Type: \"GRADIENT_BOOSTED_TREES\"\n","Task: CLASSIFICATION\n","Label: \"__LABEL\"\n","\n","Input Features (11):\n","\tAge\n","\tCabin\n","\tEmbarked\n","\tFare\n","\tName\n","\tParch\n","\tPclass\n","\tSex\n","\tSibSp\n","\tTicket_item\n","\tTicket_number\n","\n","No weights\n","\n","Variable Importance: INV_MEAN_MIN_DEPTH:\n","    1.           \"Sex\"  0.597073 ################\n","    2.           \"Age\"  0.363764 #######\n","    3.          \"Fare\"  0.264018 ###\n","    4.          \"Name\"  0.207843 #\n","    5.        \"Pclass\"  0.178906 \n","    6. \"Ticket_number\"  0.178488 \n","    7.   \"Ticket_item\"  0.177907 \n","    8.      \"Embarked\"  0.177237 \n","    9.         \"Parch\"  0.175481 \n","   10.         \"SibSp\"  0.171800 \n","\n","Variable Importance: NUM_AS_ROOT:\n","    1.  \"Sex\" 36.000000 ################\n","    2. \"Name\"  6.000000 \n","\n","Variable Importance: NUM_NODES:\n","    1.           \"Age\" 530.000000 ################\n","    2.          \"Fare\" 311.000000 #########\n","    3.          \"Name\" 66.000000 #\n","    4.   \"Ticket_item\" 50.000000 #\n","    5.           \"Sex\" 42.000000 #\n","    6.         \"Parch\" 26.000000 \n","    7. \"Ticket_number\" 21.000000 \n","    8.        \"Pclass\" 17.000000 \n","    9.      \"Embarked\" 16.000000 \n","   10.         \"SibSp\"  6.000000 \n","\n","Variable Importance: SUM_SCORE:\n","    1.           \"Sex\" 484.272240 ################\n","    2.           \"Age\" 393.999352 #############\n","    3.          \"Fare\" 323.250985 ##########\n","    4.          \"Name\" 105.330212 ###\n","    5.        \"Pclass\" 26.851849 \n","    6.   \"Ticket_item\" 25.837695 \n","    7. \"Ticket_number\" 17.652836 \n","    8.      \"Embarked\"  9.217001 \n","    9.         \"Parch\"  7.010211 \n","   10.         \"SibSp\"  0.574055 \n","\n","\n","\n","Loss: BINOMIAL_LOG_LIKELIHOOD\n","Validation loss value: 1.06082\n","Number of trees per iteration: 1\n","Node format: NOT_SET\n","Number of trees: 42\n","Total number of nodes: 2212\n","\n","Number of nodes by tree:\n","Count: 42 Average: 52.6667 StdDev: 4.47036\n","Min: 41 Max: 63 Ignored: 0\n","----------------------------------------------\n","[ 41, 42)  2   4.76%   4.76% ##\n","[ 42, 43)  0   0.00%   4.76%\n","[ 43, 44)  0   0.00%   4.76%\n","[ 44, 45)  0   0.00%   4.76%\n","[ 45, 46)  1   2.38%   7.14% #\n","[ 46, 47)  0   0.00%   7.14%\n","[ 47, 49)  2   4.76%  11.90% ##\n","[ 49, 50)  5  11.90%  23.81% ####\n","[ 50, 51)  0   0.00%  23.81%\n","[ 51, 52)  4   9.52%  33.33% ###\n","[ 52, 53)  0   0.00%  33.33%\n","[ 53, 54) 13  30.95%  64.29% ##########\n","[ 54, 55)  0   0.00%  64.29%\n","[ 55, 57)  9  21.43%  85.71% #######\n","[ 57, 58)  1   2.38%  88.10% #\n","[ 58, 59)  0   0.00%  88.10%\n","[ 59, 60)  3   7.14%  95.24% ##\n","[ 60, 61)  0   0.00%  95.24%\n","[ 61, 62)  1   2.38%  97.62% #\n","[ 62, 63]  1   2.38% 100.00% #\n","\n","Depth by leafs:\n","Count: 1127 Average: 4.8465 StdDev: 0.454147\n","Min: 2 Max: 5 Ignored: 0\n","----------------------------------------------\n","[ 2, 3)   1   0.09%   0.09%\n","[ 3, 4)  40   3.55%   3.64%\n","[ 4, 5)  90   7.99%  11.62% #\n","[ 5, 5] 996  88.38% 100.00% ##########\n","\n","Number of training obs by leaf:\n","Count: 1127 Average: 29.7764 StdDev: 71.9364\n","Min: 1 Max: 467 Ignored: 0\n","----------------------------------------------\n","[   1,  24) 884  78.44%  78.44% ##########\n","[  24,  47)  79   7.01%  85.45% #\n","[  47,  71)  44   3.90%  89.35%\n","[  71,  94)  19   1.69%  91.04%\n","[  94, 117)  13   1.15%  92.19%\n","[ 117, 141)  15   1.33%  93.52%\n","[ 141, 164)  24   2.13%  95.65%\n","[ 164, 187)   6   0.53%  96.18%\n","[ 187, 211)   4   0.35%  96.54%\n","[ 211, 234)   1   0.09%  96.63%\n","[ 234, 257)   1   0.09%  96.72%\n","[ 257, 281)   3   0.27%  96.98%\n","[ 281, 304)   2   0.18%  97.16%\n","[ 304, 327)   2   0.18%  97.34%\n","[ 327, 351)   2   0.18%  97.52%\n","[ 351, 374)  11   0.98%  98.49%\n","[ 374, 397)   5   0.44%  98.94%\n","[ 397, 421)   9   0.80%  99.73%\n","[ 421, 444)   1   0.09%  99.82%\n","[ 444, 467]   2   0.18% 100.00%\n","\n","Attribute in nodes:\n","\t530 : Age [NUMERICAL]\n","\t311 : Fare [NUMERICAL]\n","\t66 : Name [CATEGORICAL_SET]\n","\t50 : Ticket_item [CATEGORICAL]\n","\t42 : Sex [CATEGORICAL]\n","\t26 : Parch [NUMERICAL]\n","\t21 : Ticket_number [CATEGORICAL]\n","\t17 : Pclass [NUMERICAL]\n","\t16 : Embarked [CATEGORICAL]\n","\t6 : SibSp [NUMERICAL]\n","\n","Attribute in nodes with depth <= 0:\n","\t36 : Sex [CATEGORICAL]\n","\t6 : Name [CATEGORICAL_SET]\n","\n","Attribute in nodes with depth <= 1:\n","\t50 : Age [NUMERICAL]\n","\t36 : Sex [CATEGORICAL]\n","\t26 : Fare [NUMERICAL]\n","\t7 : Name [CATEGORICAL_SET]\n","\t5 : Pclass [NUMERICAL]\n","\t2 : Ticket_number [CATEGORICAL]\n","\n","Attribute in nodes with depth <= 2:\n","\t130 : Age [NUMERICAL]\n","\t76 : Fare [NUMERICAL]\n","\t36 : Sex [CATEGORICAL]\n","\t19 : Name [CATEGORICAL_SET]\n","\t8 : Ticket_number [CATEGORICAL]\n","\t7 : Embarked [CATEGORICAL]\n","\t6 : Pclass [NUMERICAL]\n","\t6 : Parch [NUMERICAL]\n","\t5 : Ticket_item [CATEGORICAL]\n","\n","Attribute in nodes with depth <= 3:\n","\t270 : Age [NUMERICAL]\n","\t173 : Fare [NUMERICAL]\n","\t39 : Name [CATEGORICAL_SET]\n","\t38 : Sex [CATEGORICAL]\n","\t18 : Ticket_item [CATEGORICAL]\n","\t13 : Ticket_number [CATEGORICAL]\n","\t12 : Parch [NUMERICAL]\n","\t12 : Embarked [CATEGORICAL]\n","\t9 : Pclass [NUMERICAL]\n","\t3 : SibSp [NUMERICAL]\n","\n","Attribute in nodes with depth <= 5:\n","\t530 : Age [NUMERICAL]\n","\t311 : Fare [NUMERICAL]\n","\t66 : Name [CATEGORICAL_SET]\n","\t50 : Ticket_item [CATEGORICAL]\n","\t42 : Sex [CATEGORICAL]\n","\t26 : Parch [NUMERICAL]\n","\t21 : Ticket_number [CATEGORICAL]\n","\t17 : Pclass [NUMERICAL]\n","\t16 : Embarked [CATEGORICAL]\n","\t6 : SibSp [NUMERICAL]\n","\n","Condition type in nodes:\n","\t890 : ObliqueCondition\n","\t145 : ContainsBitmapCondition\n","\t50 : ContainsCondition\n","Condition type in nodes with depth <= 0:\n","\t40 : ContainsBitmapCondition\n","\t2 : ContainsCondition\n","Condition type in nodes with depth <= 1:\n","\t81 : ObliqueCondition\n","\t43 : ContainsBitmapCondition\n","\t2 : ContainsCondition\n","Condition type in nodes with depth <= 2:\n","\t218 : ObliqueCondition\n","\t62 : ContainsBitmapCondition\n","\t13 : ContainsCondition\n","Condition type in nodes with depth <= 3:\n","\t467 : ObliqueCondition\n","\t89 : ContainsBitmapCondition\n","\t31 : ContainsCondition\n","Condition type in nodes with depth <= 5:\n","\t890 : ObliqueCondition\n","\t145 : ContainsBitmapCondition\n","\t50 : ContainsCondition\n","\n","Training logs:\n","Number of iteration to final model: 42\n","\tIter:1 train-loss:1.264594 valid-loss:1.360749  train-accuracy:0.624531 valid-accuracy:0.543478\n","\tIter:2 train-loss:1.210623 valid-loss:1.320363  train-accuracy:0.624531 valid-accuracy:0.543478\n","\tIter:3 train-loss:1.160657 valid-loss:1.281972  train-accuracy:0.624531 valid-accuracy:0.543478\n","\tIter:4 train-loss:1.116982 valid-loss:1.250548  train-accuracy:0.624531 valid-accuracy:0.543478\n","\tIter:5 train-loss:1.075170 valid-loss:1.221467  train-accuracy:0.807259 valid-accuracy:0.760870\n","\tIter:6 train-loss:1.035656 valid-loss:1.199482  train-accuracy:0.822278 valid-accuracy:0.760870\n","\tIter:16 train-loss:0.787670 valid-loss:1.088161  train-accuracy:0.903630 valid-accuracy:0.771739\n","\tIter:26 train-loss:0.648139 valid-loss:1.066864  train-accuracy:0.921151 valid-accuracy:0.782609\n","\tIter:36 train-loss:0.559101 valid-loss:1.068122  train-accuracy:0.921151 valid-accuracy:0.782609\n","\tIter:46 train-loss:0.496837 valid-loss:1.064204  train-accuracy:0.929912 valid-accuracy:0.771739\n","\tIter:56 train-loss:0.451017 valid-loss:1.083011  train-accuracy:0.941176 valid-accuracy:0.771739\n","\tIter:66 train-loss:0.415965 valid-loss:1.105307  train-accuracy:0.946183 valid-accuracy:0.771739\n","\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010729,"end_time":"2023-05-18T10:31:10.681492","exception":false,"start_time":"2023-05-18T10:31:10.670763","status":"completed"},"tags":[]},"source":["# Make predictions"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T02:28:32.999173Z","iopub.status.busy":"2023-11-08T02:28:32.998589Z","iopub.status.idle":"2023-11-08T02:28:34.282392Z","shell.execute_reply":"2023-11-08T02:28:34.280631Z","shell.execute_reply.started":"2023-11-08T02:28:32.999132Z"},"papermill":{"duration":1.316069,"end_time":"2023-05-18T10:31:12.008967","exception":false,"start_time":"2023-05-18T10:31:10.692898","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Submission exported to Dataset/submission.csv\n","head: cannot open '/kaggle/working/submission.csv' for reading: No such file or directory\n"]}],"source":["def prediction_to_kaggle_format(model, threshold=0.5):\n","    proba_survive = model.predict(serving_ds, verbose=0)[:,0]\n","    return pd.DataFrame({\n","        \"PassengerId\": serving_df[\"PassengerId\"],\n","        \"Survived\": (proba_survive >= threshold).astype(int)\n","    })\n","\n","def make_submission(kaggle_predictions):\n","    try:\n","        path=\"/kaggle/working/submission.csv\"\n","        kaggle_predictions.to_csv(path, index=False)\n","        print(f\"Submission exported to {path}\")\n","    except:\n","        path=\"Dataset/submission.csv\"\n","        kaggle_predictions.to_csv(path, index=False)\n","        print(f\"Submission exported to {path}\")\n","kaggle_predictions = prediction_to_kaggle_format(model)\n","make_submission(kaggle_predictions)\n","try:\n","    !head /kaggle/working/submission.csv\n","except:\n","    !head Dataset/submission.csv\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011739,"end_time":"2023-05-18T10:31:12.032772","exception":false,"start_time":"2023-05-18T10:31:12.021033","status":"completed"},"tags":[]},"source":["# Training a model with hyperparameter tunning\n","\n","Hyper-parameter tuning is enabled by specifying the tuner constructor argument of the model. The tuner object contains all the configuration of the tuner (search space, optimizer, trial and objective).\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T02:28:41.011498Z","iopub.status.busy":"2023-11-08T02:28:41.010976Z"},"papermill":{"duration":129.205111,"end_time":"2023-05-18T10:33:21.248657","exception":false,"start_time":"2023-05-18T10:31:12.043546","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Use /tmp/tmp3ne5pmlr as temporary training directory\n"]},{"name":"stderr","output_type":"stream","text":["[WARNING 23-11-08 15:38:22.4455 NZDT gradient_boosted_trees.cc:1818] \"goss_alpha\" set but \"sampling_method\" not equal to \"GOSS\".\n","[WARNING 23-11-08 15:38:22.4455 NZDT gradient_boosted_trees.cc:1829] \"goss_beta\" set but \"sampling_method\" not equal to \"GOSS\".\n","[WARNING 23-11-08 15:38:22.4455 NZDT gradient_boosted_trees.cc:1843] \"selective_gradient_boosting_ratio\" set but \"sampling_method\" not equal to \"SELGB\".\n"]}],"source":["tuner = tfdf.tuner.RandomSearch(num_trials=1000)\n","tuner.choice(\"min_examples\", [2, 5, 7, 10])\n","tuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\n","\n","local_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\n","local_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\n","\n","global_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\n","global_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\n","\n","#tuner.choice(\"use_hessian_gain\", [True, False])\n","tuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\n","tuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\n","\n","\n","tuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\n","oblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\n","oblique_space.choice(\"sparse_oblique_normalization\",\n","                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\n","oblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\n","oblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])\n","\n","# Tune the model. Notice the `tuner=tuner`.\n","tuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\n","tuned_model.fit(train_ds, verbose=0)\n","\n","tuned_self_evaluation = tuned_model.make_inspector().evaluation()\n","print(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss:{tuned_self_evaluation.loss}\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011276,"end_time":"2023-05-18T10:33:21.271148","exception":false,"start_time":"2023-05-18T10:33:21.259872","status":"completed"},"tags":[]},"source":["In the last line in the cell above, you can see the accuracy is higher than previously with default parameters and parameters set by hand.\n","\n","This is the main idea behing hyperparameter tuning.\n","\n","For more information you can follow this tutorial: [Automated hyper-parameter tuning](https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010908,"end_time":"2023-05-18T10:33:21.293210","exception":false,"start_time":"2023-05-18T10:33:21.282302","status":"completed"},"tags":[]},"source":["# Making an ensemble\n","\n","Here you'll create 100 models with different seeds and combine their results\n","\n","This approach removes a little bit the random aspects related to creating ML models\n","\n","In the GBT creation is used the `honest` parameter. It will use different training examples to infer the structure and the leaf values. This regularization technique trades examples for bias estimates."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-08T02:23:42.196047Z","iopub.status.idle":"2023-11-08T02:23:42.196505Z","shell.execute_reply":"2023-11-08T02:23:42.196317Z","shell.execute_reply.started":"2023-11-08T02:23:42.196296Z"},"papermill":{"duration":152.142847,"end_time":"2023-05-18T10:35:53.447145","exception":false,"start_time":"2023-05-18T10:33:21.304298","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["predictions = None\n","num_predictions = 0\n","\n","for i in range(100):\n","    print(f\"i:{i}\")\n","    # Possible models: GradientBoostedTreesModel or RandomForestModel\n","    model = tfdf.keras.GradientBoostedTreesModel(\n","        verbose=0, # Very few logs\n","        features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n","        exclude_non_specified_features=True, # Only use the features in \"features\"\n","\n","        #min_examples=1,\n","        #categorical_algorithm=\"RANDOM\",\n","        ##max_depth=4,\n","        #shrinkage=0.05,\n","        ##num_candidate_attributes_ratio=0.2,\n","        #split_axis=\"SPARSE_OBLIQUE\",\n","        #sparse_oblique_normalization=\"MIN_MAX\",\n","        #sparse_oblique_num_projections_exponent=2.0,\n","        #num_trees=2000,\n","        ##validation_ratio=0.0,\n","        random_seed=i,\n","        honest=True,\n","    )\n","    model.fit(train_ds)\n","    \n","    sub_predictions = model.predict(serving_ds, verbose=0)[:,0]\n","    if predictions is None:\n","        predictions = sub_predictions\n","    else:\n","        predictions += sub_predictions\n","    num_predictions += 1\n","\n","predictions/=num_predictions\n","\n","kaggle_predictions = pd.DataFrame({\n","        \"PassengerId\": serving_df[\"PassengerId\"],\n","        \"Survived\": (predictions >= 0.5).astype(int)\n","    })\n","\n","make_submission(kaggle_predictions)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.028523,"end_time":"2023-05-18T10:35:53.504450","exception":false,"start_time":"2023-05-18T10:35:53.475927","status":"completed"},"tags":[]},"source":["# What is next\n","\n","If you want to learn more about TensorFlow Decision Forests and its advanced features, you can follow the official documentation [here](https://www.tensorflow.org/decision_forests) "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"papermill":{"default_parameters":{},"duration":324.686677,"end_time":"2023-05-18T10:35:56.847460","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-05-18T10:30:32.160783","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}
